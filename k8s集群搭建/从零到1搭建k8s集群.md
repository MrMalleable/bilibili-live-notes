## 从零到1安装k8s集群

环境：Windows 10 内存32G

软件：安装集群前须安装Vagrant、VirtualBox，且下载好ubuntu的镜像

这里下载镜像我是使用中国国内的镜像源的，如果使用官方的镜像源下载镜像的话估计会疯掉。

```sh
# ubuntu 18.04 LTS:
vagrant box add https://mirrors.tuna.tsinghua.edu.cn/ubuntu-cloud-images/bionic/current/bionic-server-cloudimg-amd64-vagrant.box --name ubuntu18

# ubunt 16.04 LTS：
vagrant box add https://mirrors.tuna.tsinghua.edu.cn/ubuntu-cloud-images/xenial/current/xenial-server-cloudimg-amd64-vagrant.box --name ubuntu16

# ubuntu14：
vagrant box add https://mirrors.tuna.tsinghua.edu.cn/ubuntu-cloud-images/vagrant/trusty/current/trusty-server-cloudimg-amd64-vagrant-disk1.box --name ubuntu14
```

我这边下载的bionic镜像。



那就让我们开始吧。



### 前言

要安装K8S集群，计划使用三台机器，分别命名为master、slave1和slave2。



### 1、编写vagrantfile

三台机器的配置基本上差不多，只需要改变一下三台的内网地址。

master: 192.168.5.8

slave1: 192.168.5.9

slave2: 192.168.5.10

```ruby
# -*- mode: ruby -*-
# vi: set ft=ruby :

# All Vagrant configuration is done below. The "2" in Vagrant.configure
# configures the configuration version (we support older styles for
# backwards compatibility). Please don't change it unless you know what
# you're doing.
Vagrant.configure("2") do |config|
  # The most common configuration options are documented and commented below.
  # For a complete reference, please see the online documentation at
  # https://docs.vagrantup.com.

  # Every Vagrant development environment requires a box. You can search for
  # boxes at https://vagrantcloud.com/search.
  config.vm.box = "ubuntu/bionic"

  # Disable automatic box update checking. If you disable this, then
  # boxes will only be checked for updates when the user runs
  # `vagrant box outdated`. This is not recommended.
  # config.vm.box_check_update = false

  # Create a forwarded port mapping which allows access to a specific port
  # within the machine from a port on the host machine. In the example below,
  # accessing "localhost:8080" will access port 80 on the guest machine.
  # NOTE: This will enable public access to the opened port
  # config.vm.network "forwarded_port", guest: 80, host: 8080

  # Create a forwarded port mapping which allows access to a specific port
  # within the machine from a port on the host machine and only allow access
  # via 127.0.0.1 to disable public access
  # config.vm.network "forwarded_port", guest: 80, host: 8080, host_ip: "127.0.0.1"

  # Create a private network, which allows host-only access to the machine
  # using a specific IP.
  # config.vm.network "private_network", ip: "192.168.33.10"

  # Create a public network, which generally matched to bridged network.
  # Bridged networks make the machine appear as another physical device on
  # your network.
  config.vm.network "public_network", ip: "192.168.5.8"

  # Share an additional folder to the guest VM. The first argument is
  # the path on the host to the actual folder. The second argument is
  # the path on the guest to mount the folder. And the optional third
  # argument is a set of non-required options.
  # config.vm.synced_folder "../data", "/vagrant_data"

  # Provider-specific configuration so you can fine-tune various
  # backing providers for Vagrant. These expose provider-specific options.
  # Example for VirtualBox:
  #
  # config.vm.provider "virtualbox" do |vb|
  #   # Display the VirtualBox GUI when booting the machine
  #   vb.gui = true
  #
  #   # Customize the amount of memory on the VM:
  #   vb.memory = "4096"
  # end
  #
  # View the documentation for the provider you are using for more
  # information on available options.

  # Enable provisioning with a shell script. Additional provisioners such as
  # Ansible, Chef, Docker, Puppet and Salt are also available. Please see the
  # documentation for more information about their specific syntax and use.
  # config.vm.provision "shell", inline: <<-SHELL
  #   apt-get update
  #   apt-get install -y apache2
  # SHELL
  config.vm.provider "virtualbox" do |v|
    v.memory = 4096
    v.cpus = 2
  end
end
```

在上面的配置文件里面，我们指定了使用的镜像名称，对外的访问IP和内存以及CPU的核数。



在三个文件夹，打开命令行执行：

```sh
vagrant up
```

即可。

为验证是否启动成功，我们可以打开本地的VirtualBox来查看下。

![image-20211018200612714](C:\Users\ilovewl\AppData\Roaming\Typora\typora-user-images\image-20211018200612714.png)

这样我们就相当于启动了三台安装ubuntu系统的linux主机。



对了，启动完成之后记得看下机器的22端口映射到本机的哪个端口了，xshell连接机器的时候需要使用到。



### 使用xshell连接三台机器

这里要注意下，在没有配置机器的ssh时，我们可以通过vagrant ssh直接登录到机器，但是windows的命令行用起来不方便，所以使用xshell连接。但是由于我不知道机器的密码，所以只能通过私钥来登录，在启动之后在当前文件夹路径下可以找到`\.vagrant\machines\default\virtualbox\private_key`，我们可以使用这个私钥来登录，具体使用方法如下：

![image-20211019220427356](C:\Users\ilovewl\AppData\Roaming\Typora\typora-user-images\image-20211019220427356.png)



登录三台机器之后，我们需要修改一下三台机器的主机名（记得是在三台机器分别执行下面的某一条）：

```sh
sudo hostname master
sudo hostname slave1
sudo hostname slave2
```

修改完得重新用xshell连接下才能看到生效。



### 开始安装k8s所需要的环境

- 更换镜像源为阿里源

  因为我们安装的ubuntu系统是bionic，所以我们百度的时候需要搜索bionic对应的镜像源，贴一个网上的阿里源，替换/etc/opt/sources.list这个文件的内容，一般替换之前先把原来的备份一波哦：

  ```sh
  #阿里源
  deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse
  deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse
  deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse
  deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse
  deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse
  deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse
  deb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse
  deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse
  deb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse
  deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse
  ```

  更新完镜像源之后我们执行下以下命令将系统的软件更新一下：

  ```sh
  sudo apt-get update
  sudo apt-get upgrade
  ```

  

- 更新每台机器的hosts文件

  修改/etc/hosts文件，将另外两台机器的IP和映射添加之后保存，这里以master机器上做示范：

  ```sh
  # /etc/hosts文件
  ....
  192.168.5.9 slave1
  192.168.5.10 slave2
  ```



下面这些命令建议使用xshell的功能将敲入的命令同时发送到三个窗口更快一点，不用每台机器都去敲一遍。



- 关闭防火墙

  ```sh
  sudo ufw disable
  ```

- 关闭selinux

  ```sh
  sudo apt install selinux-utils
  sudo setenforce 0
  ```

- 关闭系统的交换区

  ```sh
  sudo swapoff -a
  ```

- 设置网络相关

  ```sh
  # 开启ipv4转发
  sudo vim /etc/sysctl.conf
  # 里面有个net ipv4 forward=1的这一行把注释取消
  sudo sysctl -p
  
  # 防火墙修改FORWARD链默认策略
  sudo iptables -P FORWARD ACCEPT
  
  # 配置iptables参数，使得流经网桥的流量也经过iptables/netfilter防火墙
  sudo tee /etc/sysctl.d/k8s.conf <<-'EOF'
  net.bridge.bridge-nf-call-ip6tables = 1
  net.bridge.bridge-nf-call-iptables = 1
  EOF
  
  sudo sysctl --system
  ```

- 安装docker(按照官网安装)

  ```sh
  sudo apt-get update
  sudo apt-get install \
      apt-transport-https \
      ca-certificates \
      curl \
      gnupg \
      lsb-release
  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
  
  echo \
    "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \
    $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
  
  sudo apt-get update
  sudo apt-get install docker-ce docker-ce-cli containerd.io
  
  # 设置docker的cgroupdriver和镜像加速
  sudo tee /etc/docker/daemon.json <<- 'EOF'
  {
  
  "exec-opts": ["native.cgroupdriver=systemd"],
  
  "registry-mirrors": ["https://5xcgs6ii.mirror.aliyuncs.com"]
  }
  EOF
  
  # 设置docker开机自启动并且启动docker
  sudo systemctl enable docker && sudo systemctl start docker
  ```

- 安装指定版本的kubernetes

  ```sh
  sudo apt-get update && sudo apt-get install -y apt-transport-https curl
  
  sudo curl -s https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add -
  
  sudo tee /etc/apt/sources.list.d/kubernetes.list <<-'EOF'
  deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial main
  EOF
  
  sudo apt update
  
  sudo apt-get install -y kubelet=1.22.2-00 kubeadm=1.22.2-00 kubectl=1.22.2-00
  sudo apt-mark hold kubelet=1.22.2-00 kubeadm=1.22.2-00 kubectl=1.22.2-00
  
  sudo systemctl enable kubelet && sudo systemctl start kubelet
  ```

- 查看启动K8S集群需要的docker镜像

  ```sh
  sudo kubeadm config images list --kubernetes-version=v1.22.2
  ```

  我这边的结果是：

  ```sh
  k8s.gcr.io/kube-apiserver:v1.22.2
  k8s.gcr.io/kube-controller-manager:v1.22.2
  k8s.gcr.io/kube-scheduler:v1.22.2
  k8s.gcr.io/kube-proxy:v1.22.2
  k8s.gcr.io/pause:3.5
  k8s.gcr.io/etcd:3.5.0-0
  k8s.gcr.io/coredns/coredns:v1.8.4
  ```

  因此我们需要将上面这些镜像下载到每台机器，由于这些镜像如果从官方镜像源下载的话，可能心情会崩溃，所以这里我们从阿里云的镜像源进行下载

- 下载需要的docker镜像

  为了不重复敲命令，我们编写一个pull.sh的脚本：

  ```sh
  # vim pull.sh
  #!/bin/bash
  images=(kube-proxy:v1.22.2 kube-scheduler:v1.22.2 kube-controller-manager:v1.22.2 kube-apiserver:v1.22.2 etcd:3.5.0-0 pause:3.5 )
  for imageName in ${images[@]} ; do
  docker pull registry.aliyuncs.com/google_containers/$imageName
  docker tag registry.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName
  docker rmi registry.aliyuncs.com/google_containers/$imageName
  done
  ```

  然后执行:

  ```sh
  sudo ./pull.sh
  ```

  等待下载完成即可。

  coredns和flannel的镜像我们使用docker pull命令下载一下：

  ```sh
  sudo docker pull coredns/coredns:1.8.4
  sudo docker tag coredns/coredns:1.8.4   k8s.gcr.io/coredns/coredns:v1.8.4
  ```



这里就需要关闭发送键盘命令到三个窗口的功能了哦。



- 在master节点上启动kubernetes集群

  ```sh
  # 第一个apiserver就是master的ip地址，第二个地址下面设置flannel时会用到
  sudo kubeadm init --apiserver-advertise-address=192.168.5.8 --pod-network-cidr=172.16.0.0/16  --kubernetes-version=v1.22.2
  ```

  安装成功之后，按照提示执行：

  ```sh
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
  ```

  然后需要把最后一行命令给记下来,类似于这样：

  ```sh
  kubeadm join 192.168.33.10:6443 --token m9evqe.7pu5lxgo5dg1u74e \
  	--discovery-token-ca-cert-hash sha256:8cfa362bc5bec12e1fca178aa549d8445cd1095da812b96f8318d04b4151da03
  ```

- 分别在slave1和slave2上面执行上面最后一行命令

  ```sh
  kubeadm join 192.168.33.10:6443 --token m9evqe.7pu5lxgo5dg1u74e \
  	--discovery-token-ca-cert-hash sha256:8cfa362bc5bec12e1fca178aa549d8445cd1095da812b96f8318d04b4151da03
  ```

  执行到这的话，基本的集群搭建就完成了哦



### 测试集群搭建情况

在master节点执行以下命令：

```sh
sudo kubectl get nodes
```



你可以看到master、slave1和slave2三个节点的数据，但是它们的状态都是NotReady,这是为什么呢？



我们下一篇文章再来详细讲一下吧，我实在写不动了，困死了，如果你看到这，别忘记一键三连哦，我会继续努力的！



